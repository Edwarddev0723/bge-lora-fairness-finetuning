{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fair LoRA Model Training\n",
    "\n",
    "é€™å€‹ notebook ä½¿ç”¨å…¬å¹³æ€§å¢žå¼·çš„ LoRA å¾®èª¿æ–¹æ³• (Base: BAAI/bge-large-en-v1.5) ä¾†é€²è¡Œå±¥æ­·èˆ‡è·ä½æè¿°é…å°ï¼ŒåŒæ™‚ç›¡é‡é™ä½Žä¸åŒæ•™è‚²èƒŒæ™¯ç¾¤çµ„ä¹‹é–“çš„é æ¸¬å·®ç•°ã€‚\n",
    "\n",
    "è³‡æ–™ä¾†æºï¼šç¾åœ¨å·²æ›´æ–°ç‚ºé‡æ–°åˆ†å‰²å¾Œçš„è³‡æ–™é›† `processed_resume_dataset_resplit`ï¼Œç¢ºä¿åœ¨ train/val/test ä¸­éƒ½å­˜åœ¨æ­£ä¾‹ï¼Œé¿å…å…¬å¹³æ€§æŒ‡æ¨™é€€åŒ–ã€‚\n",
    "\n",
    "## ç›®æ¨™\n",
    "\n",
    "- ä¸»ä»»å‹™ï¼šé æ¸¬å±¥æ­·èˆ‡è·ä½æ˜¯å¦åŒ¹é… (Fit / No Fit)\n",
    "- æ•æ„Ÿå±¬æ€§ï¼š`is_top_school`, `school_category`\n",
    "- å…¬å¹³æ€§ç­–ç•¥ï¼š\n",
    "  - æ•æ„Ÿå±¬æ€§å±è”½ï¼ˆæ–‡å­—æ›¿æ›ç‚ºä¸­æ€§æ¨™è¨˜ï¼‰\n",
    "  - LoRA å¾®èª¿ (r=8, alpha=16, target_modules=['query','key','value'])\n",
    "  - å°æŠ—å¼åŽ»å (Adversarial Debiasing)\n",
    "  - å¤šä»»å‹™å­¸ç¿’ (è¼”åŠ©é æ¸¬ school_category ä»¥æ­£å‰‡åŒ–è¡¨ç¤º)\n",
    "  - åŠ æ¬Š/é‡æŠ½æ¨£æ¸›å°‘ç¾¤é«”ä¸å¹³è¡¡\n",
    "  - å…¬å¹³æ€§æŒ‡æ¨™ç›£æŽ§èˆ‡æ—©åœ (Demographic Parity / Equalized Odds / Equal Opportunity)\n",
    "\n",
    "## æµç¨‹\n",
    "1. ç’°å¢ƒèˆ‡åƒæ•¸è¨­ç½®\n",
    "2. æ•¸æ“šåŠ è¼‰èˆ‡æ•æ„Ÿå±¬æ€§å±è”½ï¼ˆä½¿ç”¨æ–°çš„é‡æ–°åˆ†å‰²è³‡æ–™é›†ï¼‰\n",
    "3. å»ºç«‹æ¨¡åž‹èˆ‡ LoRA é…ç½®\n",
    "4. è¨“ç·´è¿´åœˆï¼ˆåŒ…å«å°æŠ— + å…¬å¹³æ€§æå¤±ï¼‰\n",
    "5. è©•ä¼°èˆ‡å…¬å¹³æ€§æŒ‡æ¨™è¼¸å‡ºï¼ˆæ”¯æ´å¾ŒçºŒé–¾å€¼å¾®èª¿ï¼‰\n",
    "6. ä¿å­˜æœ€ä½³æ¨¡åž‹èˆ‡è¨“ç·´è¨˜éŒ„\n",
    "\n",
    "åŸ·è¡Œ Cells 1â†’N å®Œæˆæ•´é«”è¨“ç·´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Dataset path: /Users/edwardhuang/Documents/GitHub/bge-lora-fairness-finetuning/data/processed/processed_resume_dataset_resplit\n",
      "GroupBatchSampler enabled: False\n",
      "Balanced validation enabled: True\n",
      "Fairness regularizer lambda: 0.05\n",
      "Temperature calibration: True\n",
      "AUC metrics enabled: True\n"
     ]
    }
   ],
   "source": [
    "# Imports & configuration (updated for new sampler/balanced validation)\n",
    "import sys, json\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path('.').resolve()\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "from fair_lora_config import (\n",
    "    BASE_MODEL, DATASET_PATH, DEVICE, LEARNING_RATE, MAX_GRAD_NORM,\n",
    "    ADVERSARIAL_LAMBDA, FAIRNESS_LAMBDA, MULTITASK_LAMBDA,\n",
    "    USE_GROUP_BATCH_SAMPLER, USE_REWEIGHTING, CREATE_BALANCED_VAL,\n",
    "    FAIRNESS_REG_LAMBDA, WINDOW_SELECTION_START, WINDOW_SELECTION_END,\n",
    "    WINDOW_FAIRNESS_THRESHOLD, TEMPERATURE_CALIBRATION, AUC_ENABLED\n",
    ")\n",
    "\n",
    "from src.fair_data_loader import create_data_loaders\n",
    "from src.fair_lora_model import FairLoRAModel\n",
    "from src.fair_trainer import FairTrainer\n",
    "import torch\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"GroupBatchSampler enabled: {USE_GROUP_BATCH_SAMPLER}\")\n",
    "print(f\"Balanced validation enabled: {CREATE_BALANCED_VAL}\")\n",
    "print(f\"Fairness regularizer lambda: {FAIRNESS_REG_LAMBDA}\")\n",
    "print(f\"Temperature calibration: {TEMPERATURE_CALIBRATION}\")\n",
    "print(f\"AUC metrics enabled: {AUC_ENABLED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip: use the enhanced create_data_loaders call in the next cell (returns 5 values).\n"
     ]
    }
   ],
   "source": [
    "# (Deprecated) Initial simple loader call replaced by enhanced call in next cell.\n",
    "# Keeping this cell as a placeholder to avoid unpack errors.\n",
    "print(\"Skip: use the enhanced create_data_loaders call in the next cell (returns 5 values).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ Loading dataset from /Users/edwardhuang/Documents/GitHub/bge-lora-fairness-finetuning/data/processed/processed_resume_dataset_resplit...\n",
      "   Train: 5,472 samples\n",
      "   Val:   685 samples\n",
      "   Test:  1,087 samples\n",
      "\n",
      "ðŸ“Š School Distribution:\n",
      "\n",
      "   Train:\n",
      "     no_school_mentioned...........   884 (16.15%)\n",
      "     non_top_school................  4365 (79.77%)\n",
      "     top_school....................   223 ( 4.08%)\n",
      "\n",
      "   Val:\n",
      "     non_top_school................   547 (79.85%)\n",
      "     no_school_mentioned...........   111 (16.20%)\n",
      "     top_school....................    27 ( 3.94%)\n",
      "\n",
      "   Test:\n",
      "     non_top_school................   867 (79.76%)\n",
      "     no_school_mentioned...........   176 (16.19%)\n",
      "     top_school....................    44 ( 4.05%)\n",
      "\n",
      "ðŸ”¤ Loading tokenizer: BAAI/bge-large-en-v1.5\n",
      "\n",
      "âš–ï¸  Using weighted sampling for fairness...\n",
      "\n",
      "ðŸ§ª Balanced Val created (joint groups): 81 samples; per-group ~27\n",
      "\n",
      "âœ… Data loaders created successfully!\n",
      "   Train batches: 684\n",
      "   Val batches:   86\n",
      "   Test batches:  136\n",
      "   Balanced Val batches: 11\n",
      "Train batches: 684 | Val batches: 86 | Test batches: 136\n",
      "Balanced Val loader: present\n",
      "\n",
      "âš–ï¸  Using weighted sampling for fairness...\n",
      "\n",
      "ðŸ§ª Balanced Val created (joint groups): 81 samples; per-group ~27\n",
      "\n",
      "âœ… Data loaders created successfully!\n",
      "   Train batches: 684\n",
      "   Val batches:   86\n",
      "   Test batches:  136\n",
      "   Balanced Val batches: 11\n",
      "Train batches: 684 | Val batches: 86 | Test batches: 136\n",
      "Balanced Val loader: present\n"
     ]
    }
   ],
   "source": [
    "# Load data with new sampler + balanced validation loader\n",
    "(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    tokenizer,\n",
    "    balanced_val_loader\n",
    ") = create_data_loaders(\n",
    "    dataset_path=str(DATASET_PATH),\n",
    "    batch_size=8,  # reduced from 16 to 8 to mitigate MPS OOM\n",
    "    max_seq_length=256,  # limit sequence length to 256 (try 384 if quality drops)\n",
    "    use_reweighting=USE_REWEIGHTING and not USE_GROUP_BATCH_SAMPLER,  # avoid double fairness intervention\n",
    "    use_group_batch_sampler=USE_GROUP_BATCH_SAMPLER,\n",
    "    create_balanced_val=CREATE_BALANCED_VAL,\n",
    "    num_workers=2 if DEVICE.type == 'mps' else 4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)} | Test batches: {len(test_loader)}\")\n",
    "print(f\"Balanced Val loader: {'present' if balanced_val_loader is not None else 'none'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Loading base model: BAAI/bge-large-en-v1.5\n",
      "ðŸ”§ Applying LoRA (expanded targets, r=16, alpha=32)...\n",
      "   â€¢ LoRA target modules detected: ['dense', 'key', 'query', 'value']\n",
      "     (modules not present are ignored silently by PEFT)\n",
      "ðŸ”’ Frozen encoder base weights except attention in last 4 layers; LoRA adapters remain trainable.\n",
      "ðŸ“Š Trainable params -> LoRA: 7,110,656 | Base last4(attn): 16,801,792 | Total: 23,912,448\n",
      "trainable params: 23,912,448 || all params: 342,252,544 || trainable%: 6.9868\n",
      "ðŸ”§ Adding adversarial discriminator...\n",
      "ðŸ”§ Adding attribute classifier for multi-task learning...\n",
      "ðŸ”§ Applying LoRA (expanded targets, r=16, alpha=32)...\n",
      "   â€¢ LoRA target modules detected: ['dense', 'key', 'query', 'value']\n",
      "     (modules not present are ignored silently by PEFT)\n",
      "ðŸ”’ Frozen encoder base weights except attention in last 4 layers; LoRA adapters remain trainable.\n",
      "ðŸ“Š Trainable params -> LoRA: 7,110,656 | Base last4(attn): 16,801,792 | Total: 23,912,448\n",
      "trainable params: 23,912,448 || all params: 342,252,544 || trainable%: 6.9868\n",
      "ðŸ”§ Adding adversarial discriminator...\n",
      "ðŸ”§ Adding attribute classifier for multi-task learning...\n",
      "Gradient checkpointing enabled.\n",
      "Gradient checkpointing enabled.\n"
     ]
    }
   ],
   "source": [
    "# Build model with optional gradient checkpointing for memory savings\n",
    "model = FairLoRAModel(\n",
    "    base_model_name=BASE_MODEL,\n",
    "    use_lora=True,\n",
    "    use_adversarial=True,\n",
    "    use_multitask=True,\n",
    "    num_labels=2\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Enable gradient checkpointing if available (helps reduce memory footprint)\n",
    "try:\n",
    "    base_attr = None\n",
    "    for attr_name in ['base_model','model']:\n",
    "        if hasattr(model, attr_name):\n",
    "            candidate = getattr(model, attr_name)\n",
    "            if hasattr(candidate, 'gradient_checkpointing_enable'):\n",
    "                candidate.gradient_checkpointing_enable()\n",
    "                base_attr = candidate\n",
    "                break\n",
    "    if base_attr is not None:\n",
    "        print(\"Gradient checkpointing enabled.\")\n",
    "    else:\n",
    "        print(\"Gradient checkpointing method not found; skipped.\")\n",
    "except Exception as e:\n",
    "    print(f\"Gradient checkpointing enabling failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready.\n"
     ]
    }
   ],
   "source": [
    "# Build trainer with balanced val and fairness reg + temperature/AUC toggles\n",
    "trainer = FairTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    balanced_val_loader=balanced_val_loader,\n",
    "    device=str(DEVICE),\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    adversarial_lambda=ADVERSARIAL_LAMBDA,\n",
    "    fairness_lambda=FAIRNESS_LAMBDA,\n",
    "    multitask_lambda=MULTITASK_LAMBDA,\n",
    "    fairness_reg_lambda=FAIRNESS_REG_LAMBDA,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    temperature_calibration=TEMPERATURE_CALIBRATION,\n",
    "    auc_enabled=AUC_ENABLED\n",
    ")\n",
    "print(\"Trainer ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 10260; Warmup steps (lambda ramp): 1026\n",
      "Using CosineAnnealingWarmRestarts with T_0=2 epochs (~1368 steps), T_mult=2.\n"
     ]
    }
   ],
   "source": [
    "# Scheduler + training loop params\n",
    "num_epochs = 15\n",
    "warmup_steps = int(0.1 * num_epochs * len(train_loader))  # 10% of total steps linear ramp\n",
    "patience = 5\n",
    "\n",
    "# CosineAnnealingWarmRestarts with restarts every ~2 epochs (measured in steps)\n",
    "import math\n",
    "steps_per_epoch = len(train_loader)\n",
    "t0_epochs = 2\n",
    "t0_steps = t0_epochs * steps_per_epoch  # since we step scheduler per batch\n",
    "eta_min = 0.0  # minimum LR at cosine trough; can raise slightly if needed\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    trainer.main_optimizer,\n",
    "    T_0=t0_steps,\n",
    "    T_mult=2,  # progressively longer cycles\n",
    "    eta_min=eta_min\n",
    ")\n",
    "print(f\"Total steps: {num_epochs * steps_per_epoch}; Warmup steps (lambda ramp): {warmup_steps}\")\n",
    "print(f\"Using CosineAnnealingWarmRestarts with T_0={t0_epochs} epochs (~{t0_steps} steps), T_mult=2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: mps\n",
      "Training batches: 684\n",
      "Validation batches: 86\n",
      "Adversarial debiasing: True\n",
      "Multi-task learning: True\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/684 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 1 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [05:42<00:00,  2.00it/s, loss=0.8302, acc=55.77%, advÎ»=0.333, mtÎ»=0.133]\n",
      "Epoch 1 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [05:42<00:00,  2.00it/s, loss=0.8302, acc=55.77%, advÎ»=0.333, mtÎ»=0.133]\n",
      "Epoch 1 [Val]:   0%|          | 0/86 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:43<00:00,  1.96it/s, loss=0.5872]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Temperature Scaling] Calibrated temperature: 2.2736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Val-Balanced]:   0%|          | 0/11 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1 [Val-Balanced]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:18<00:00,  1.73s/it, loss=1.1312]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n",
      "Train - Loss: 0.8339, Acc: 0.5577\n",
      "Val   - Loss: 0.6955, Acc: 0.5109\n",
      "Acc (thr=0.5 raw): 0.5314 | Acc (tuned): 0.5109\n",
      "Fairness (thr=0.5) - Raw: 0.1741 | Balanced: 0.2275\n",
      "Balanced Loader Fairness: 0.1763\n",
      "                 - Demographic Parity Diff: 0.1632\n",
      "                 - Equalized Odds Diff: 0.1656\n",
      "Group positive rates (school_category): {0: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.37037037037037035, 'pred_pos_rate_tuned_thr': 1.0}, 1: {'count': 547, 'true_pos_rate': 0.5027422303473492, 'pred_pos_rate_fairness_thr': 0.25411334552102377, 'pred_pos_rate_tuned_thr': 0.9817184643510055}, 2: {'count': 111, 'true_pos_rate': 0.4954954954954955, 'pred_pos_rate_fairness_thr': 0.2072072072072072, 'pred_pos_rate_tuned_thr': 0.918918918918919}}\n",
      "Group positive rates (is_top_school): {0: {'count': 658, 'true_pos_rate': 0.5015197568389058, 'pred_pos_rate_fairness_thr': 0.24620060790273557, 'pred_pos_rate_tuned_thr': 0.9711246200607903}, 1: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.37037037037037035, 'pred_pos_rate_tuned_thr': 1.0}}\n",
      "AUC: 0.5717 | PR-AUC: 0.5636\n",
      "Fairness Reg Gap (train avg): 0.0603\n",
      "Temperature: 2.2736\n",
      "EarlyStopping score (normalized): 0.5000\n",
      "------------------------------------------------------------\n",
      "âœ“ New best model saved (score: 0.5000)\n",
      "âœ“ New best model saved (score: 0.5000)\n",
      "[Checkpoint] Saved best_fairness_model.pt (fairness=0.2275)\n",
      "[Checkpoint] Saved best_fairness_model.pt (fairness=0.2275)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]:   0%|          | 0/684 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 2 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [05:41<00:00,  2.00it/s, loss=0.9536, acc=60.67%, advÎ»=0.500, mtÎ»=0.200]\n",
      "Epoch 2 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [05:41<00:00,  2.00it/s, loss=0.9536, acc=60.67%, advÎ»=0.500, mtÎ»=0.200]\n",
      "Epoch 2 [Val]:   0%|          | 0/86 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 2 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:46<00:00,  1.85it/s, loss=0.5816]\n",
      "Epoch 2 [Val-Balanced]:   0%|          | 0/11 [00:00<?, ?it/s]\n",
      "Epoch 2 [Val-Balanced]:   0%|          | 0/11 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 2 [Val-Balanced]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:19<00:00,  1.73s/it, loss=0.9715]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/15\n",
      "Train - Loss: 0.9191, Acc: 0.6067\n",
      "Val   - Loss: 0.6888, Acc: 0.5007\n",
      "Acc (thr=0.5 raw): 0.5635 | Acc (tuned): 0.5007\n",
      "Fairness (thr=0.5) - Raw: 0.1269 | Balanced: 0.3158\n",
      "Balanced Loader Fairness: 0.1486\n",
      "                 - Demographic Parity Diff: 0.1117\n",
      "                 - Equalized Odds Diff: 0.1735\n",
      "Group positive rates (school_category): {0: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.4444444444444444, 'pred_pos_rate_tuned_thr': 1.0}, 1: {'count': 547, 'true_pos_rate': 0.5027422303473492, 'pred_pos_rate_fairness_thr': 0.4990859232175503, 'pred_pos_rate_tuned_thr': 1.0}, 2: {'count': 111, 'true_pos_rate': 0.4954954954954955, 'pred_pos_rate_fairness_thr': 0.38738738738738737, 'pred_pos_rate_tuned_thr': 1.0}}\n",
      "Group positive rates (is_top_school): {0: {'count': 658, 'true_pos_rate': 0.5015197568389058, 'pred_pos_rate_fairness_thr': 0.48024316109422494, 'pred_pos_rate_tuned_thr': 1.0}, 1: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.4444444444444444, 'pred_pos_rate_tuned_thr': 1.0}}\n",
      "AUC: 0.5738 | PR-AUC: 0.5704\n",
      "Fairness Reg Gap (train avg): 0.1217\n",
      "Temperature: 2.2736\n",
      "EarlyStopping score (normalized): 0.0000\n",
      "------------------------------------------------------------\n",
      "âœ“ New best model saved (score: 0.0000)\n",
      "âœ“ New best model saved (score: 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]:   0%|          | 0/684 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 3 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [05:53<00:00,  1.94it/s, loss=1.0113, acc=59.48%, advÎ»=0.500, mtÎ»=0.200]\n",
      "Epoch 3 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [05:53<00:00,  1.94it/s, loss=1.0113, acc=59.48%, advÎ»=0.500, mtÎ»=0.200]\n",
      "Epoch 3 [Val]:   0%|          | 0/86 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 3 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:45<00:00,  1.88it/s, loss=0.5579]\n",
      "Epoch 3 [Val-Balanced]:   0%|          | 0/11 [00:00<?, ?it/s]\n",
      "Epoch 3 [Val-Balanced]:   0%|          | 0/11 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 3 [Val-Balanced]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:18<00:00,  1.67s/it, loss=0.8968]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/15\n",
      "Train - Loss: 1.0325, Acc: 0.5948\n",
      "Val   - Loss: 0.7013, Acc: 0.5007\n",
      "Acc (thr=0.5 raw): 0.5401 | Acc (tuned): 0.5007\n",
      "Fairness (thr=0.5) - Raw: 0.1773 | Balanced: 0.2211\n",
      "Balanced Loader Fairness: 0.1751\n",
      "                 - Demographic Parity Diff: 0.1505\n",
      "                 - Equalized Odds Diff: 0.1698\n",
      "Group positive rates (school_category): {0: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.3333333333333333, 'pred_pos_rate_tuned_thr': 1.0}, 1: {'count': 547, 'true_pos_rate': 0.5027422303473492, 'pred_pos_rate_fairness_thr': 0.18281535648994515, 'pred_pos_rate_tuned_thr': 1.0}, 2: {'count': 111, 'true_pos_rate': 0.4954954954954955, 'pred_pos_rate_fairness_thr': 0.1891891891891892, 'pred_pos_rate_tuned_thr': 1.0}}\n",
      "Group positive rates (is_top_school): {0: {'count': 658, 'true_pos_rate': 0.5015197568389058, 'pred_pos_rate_fairness_thr': 0.1838905775075988, 'pred_pos_rate_tuned_thr': 1.0}, 1: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.3333333333333333, 'pred_pos_rate_tuned_thr': 1.0}}\n",
      "AUC: 0.5799 | PR-AUC: 0.5792\n",
      "Fairness Reg Gap (train avg): 0.1188\n",
      "Temperature: 2.2736\n",
      "EarlyStopping score (normalized): 0.9785\n",
      "------------------------------------------------------------\n",
      "Early stopping patience: 1/5\n",
      "Early stopping patience: 1/5\n",
      "[Checkpoint] Saved best_fairness_model.pt (fairness=0.2211)\n",
      "[Checkpoint] Saved best_fairness_model.pt (fairness=0.2211)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]:   0%|          | 0/684 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 4 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [05:38<00:00,  2.02it/s, loss=1.0009, acc=62.37%, advÎ»=0.500, mtÎ»=0.200]\n",
      "Epoch 4 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [05:38<00:00,  2.02it/s, loss=1.0009, acc=62.37%, advÎ»=0.500, mtÎ»=0.200]\n",
      "Epoch 4 [Val]:   0%|          | 0/86 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 4 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:43<00:00,  1.98it/s, loss=0.5601]\n",
      "Epoch 4 [Val-Balanced]:   0%|          | 0/11 [00:00<?, ?it/s]\n",
      "Epoch 4 [Val-Balanced]:   0%|          | 0/11 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 4 [Val-Balanced]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:18<00:00,  1.68s/it, loss=0.6005]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/15\n",
      "Train - Loss: 0.9800, Acc: 0.6237\n",
      "Val   - Loss: 0.6839, Acc: 0.5226\n",
      "Acc (thr=0.5 raw): 0.5693 | Acc (tuned): 0.5226\n",
      "Fairness (thr=0.5) - Raw: 0.1516 | Balanced: 0.2309\n",
      "Balanced Loader Fairness: 0.1478\n",
      "                 - Demographic Parity Diff: 0.2543\n",
      "                 - Equalized Odds Diff: 0.2546\n",
      "Group positive rates (school_category): {0: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.7407407407407407, 'pred_pos_rate_tuned_thr': 1.0}, 1: {'count': 547, 'true_pos_rate': 0.5027422303473492, 'pred_pos_rate_fairness_thr': 0.6325411334552102, 'pred_pos_rate_tuned_thr': 0.9305301645338209}, 2: {'count': 111, 'true_pos_rate': 0.4954954954954955, 'pred_pos_rate_fairness_thr': 0.4864864864864865, 'pred_pos_rate_tuned_thr': 0.954954954954955}}\n",
      "Group positive rates (is_top_school): {0: {'count': 658, 'true_pos_rate': 0.5015197568389058, 'pred_pos_rate_fairness_thr': 0.60790273556231, 'pred_pos_rate_tuned_thr': 0.9346504559270516}, 1: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.7407407407407407, 'pred_pos_rate_tuned_thr': 1.0}}\n",
      "AUC: 0.5976 | PR-AUC: 0.5898\n",
      "Fairness Reg Gap (train avg): 0.1480\n",
      "Temperature: 2.2736\n",
      "EarlyStopping score (normalized): 0.0000\n",
      "------------------------------------------------------------\n",
      "Early stopping patience: 2/5\n",
      "Early stopping patience: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:   0%|          | 0/684 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 5 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [05:44<00:00,  1.99it/s, loss=0.5015, acc=62.34%, advÎ»=0.500, mtÎ»=0.200]\n",
      "Epoch 5 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [05:44<00:00,  1.99it/s, loss=0.5015, acc=62.34%, advÎ»=0.500, mtÎ»=0.200]\n",
      "Epoch 5 [Val]:   0%|          | 0/86 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 5 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:46<00:00,  1.85it/s, loss=0.5184]\n",
      "Epoch 5 [Val-Balanced]:   0%|          | 0/11 [00:00<?, ?it/s]\n",
      "Epoch 5 [Val-Balanced]:   0%|          | 0/11 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 5 [Val-Balanced]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:19<00:00,  1.82s/it, loss=0.7783]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/15\n",
      "Train - Loss: 0.9191, Acc: 0.6234\n",
      "Val   - Loss: 0.6916, Acc: 0.5226\n",
      "Acc (thr=0.5 raw): 0.5679 | Acc (tuned): 0.5226\n",
      "Fairness (thr=0.5) - Raw: 0.1178 | Balanced: 0.1328\n",
      "Balanced Loader Fairness: 0.1507\n",
      "                 - Demographic Parity Diff: 0.1300\n",
      "                 - Equalized Odds Diff: 0.1550\n",
      "Group positive rates (school_category): {0: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.4074074074074074, 'pred_pos_rate_tuned_thr': 1.0}, 1: {'count': 547, 'true_pos_rate': 0.5027422303473492, 'pred_pos_rate_fairness_thr': 0.5173674588665448, 'pred_pos_rate_tuned_thr': 0.9414990859232175}, 2: {'count': 111, 'true_pos_rate': 0.4954954954954955, 'pred_pos_rate_fairness_thr': 0.38738738738738737, 'pred_pos_rate_tuned_thr': 0.8828828828828829}}\n",
      "Group positive rates (is_top_school): {0: {'count': 658, 'true_pos_rate': 0.5015197568389058, 'pred_pos_rate_fairness_thr': 0.49544072948328266, 'pred_pos_rate_tuned_thr': 0.9316109422492401}, 1: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.4074074074074074, 'pred_pos_rate_tuned_thr': 1.0}}\n",
      "AUC: 0.5958 | PR-AUC: 0.5851\n",
      "Fairness Reg Gap (train avg): 0.1614\n",
      "Temperature: 2.2736\n",
      "EarlyStopping score (normalized): 0.3412\n",
      "------------------------------------------------------------\n",
      "Early stopping patience: 3/5\n",
      "[Adv Lambda] Fairness target met with low utility -> clamp adv lambda to [0.3, 0.5] going forward.\n",
      "Early stopping patience: 3/5\n",
      "[Adv Lambda] Fairness target met with low utility -> clamp adv lambda to [0.3, 0.5] going forward.\n",
      "[Checkpoint] Saved best_fairness_model.pt (fairness=0.1328)\n",
      "[Checkpoint] Saved best_fairness_model.pt (fairness=0.1328)\n",
      "[Checkpoint] Saved best_util_model.pt (AUC=0.5958, fairness_ok=True)\n",
      "[Checkpoint] Saved best_util_model.pt (AUC=0.5958, fairness_ok=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]:   0%|          | 0/684 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 6 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [18:31<00:00,  1.62s/it, loss=0.8394, acc=63.19%, advÎ»=0.500, mtÎ»=0.200]   \n",
      "Epoch 6 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [18:31<00:00,  1.62s/it, loss=0.8394, acc=63.19%, advÎ»=0.500, mtÎ»=0.200]\n",
      "Epoch 6 [Val]:   0%|          | 0/86 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 6 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:41<00:00,  2.05it/s, loss=0.5440]\n",
      "Epoch 6 [Val-Balanced]:   0%|          | 0/11 [00:00<?, ?it/s]\n",
      "Epoch 6 [Val-Balanced]:   0%|          | 0/11 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 6 [Val-Balanced]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:18<00:00,  1.67s/it, loss=0.7019]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/15\n",
      "Train - Loss: 0.8194, Acc: 0.6319\n",
      "Val   - Loss: 0.6893, Acc: 0.5182\n",
      "Acc (thr=0.5 raw): 0.5650 | Acc (tuned): 0.5182\n",
      "Fairness (thr=0.5) - Raw: 0.0951 | Balanced: 0.1503\n",
      "Balanced Loader Fairness: 0.1519\n",
      "                 - Demographic Parity Diff: 0.1441\n",
      "                 - Equalized Odds Diff: 0.1466\n",
      "Group positive rates (school_category): {0: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.5185185185185185, 'pred_pos_rate_tuned_thr': 1.0}, 1: {'count': 547, 'true_pos_rate': 0.5027422303473492, 'pred_pos_rate_fairness_thr': 0.6215722120658135, 'pred_pos_rate_tuned_thr': 0.9597806215722121}, 2: {'count': 111, 'true_pos_rate': 0.4954954954954955, 'pred_pos_rate_fairness_thr': 0.4774774774774775, 'pred_pos_rate_tuned_thr': 0.8918918918918919}}\n",
      "Group positive rates (is_top_school): {0: {'count': 658, 'true_pos_rate': 0.5015197568389058, 'pred_pos_rate_fairness_thr': 0.5972644376899696, 'pred_pos_rate_tuned_thr': 0.9483282674772037}, 1: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.5185185185185185, 'pred_pos_rate_tuned_thr': 1.0}}\n",
      "AUC: 0.6006 | PR-AUC: 0.5912\n",
      "Fairness Reg Gap (train avg): 0.1847\n",
      "Temperature: 2.2736\n",
      "EarlyStopping score (normalized): 0.2597\n",
      "------------------------------------------------------------\n",
      "Early stopping patience: 4/5\n",
      "Early stopping patience: 4/5\n",
      "[Checkpoint] Saved best_util_model.pt (AUC=0.6006, fairness_ok=True)\n",
      "[Checkpoint] Saved best_util_model.pt (AUC=0.6006, fairness_ok=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]:   0%|          | 0/684 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 7 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [05:49<00:00,  1.96it/s, loss=0.9250, acc=61.93%, advÎ»=0.500, mtÎ»=0.200]\n",
      "Epoch 7 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [05:49<00:00,  1.96it/s, loss=0.9250, acc=61.93%, advÎ»=0.500, mtÎ»=0.200]\n",
      "Epoch 7 [Val]:   0%|          | 0/86 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 7 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:44<00:00,  1.95it/s, loss=0.4340]\n",
      "Epoch 7 [Val-Balanced]:   0%|          | 0/11 [00:00<?, ?it/s]\n",
      "Epoch 7 [Val-Balanced]:   0%|          | 0/11 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 7 [Val-Balanced]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:18<00:00,  1.69s/it, loss=1.1593]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/15\n",
      "Train - Loss: 1.0107, Acc: 0.6193\n",
      "Val   - Loss: 0.7115, Acc: 0.5328\n",
      "Acc (thr=0.5 raw): 0.5591 | Acc (tuned): 0.5328\n",
      "Fairness (thr=0.5) - Raw: 0.1608 | Balanced: 0.1643\n",
      "Balanced Loader Fairness: 0.1925\n",
      "                 - Demographic Parity Diff: 0.1091\n",
      "                 - Equalized Odds Diff: 0.1439\n",
      "Group positive rates (school_category): {0: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.37037037037037035, 'pred_pos_rate_tuned_thr': 1.0}, 1: {'count': 547, 'true_pos_rate': 0.5027422303473492, 'pred_pos_rate_fairness_thr': 0.263254113345521, 'pred_pos_rate_tuned_thr': 0.9542961608775137}, 2: {'count': 111, 'true_pos_rate': 0.4954954954954955, 'pred_pos_rate_fairness_thr': 0.26126126126126126, 'pred_pos_rate_tuned_thr': 0.9009009009009009}}\n",
      "Group positive rates (is_top_school): {0: {'count': 658, 'true_pos_rate': 0.5015197568389058, 'pred_pos_rate_fairness_thr': 0.2629179331306991, 'pred_pos_rate_tuned_thr': 0.9452887537993921}, 1: {'count': 27, 'true_pos_rate': 0.48148148148148145, 'pred_pos_rate_fairness_thr': 0.37037037037037035, 'pred_pos_rate_tuned_thr': 1.0}}\n",
      "AUC: 0.6183 | PR-AUC: 0.6123\n",
      "Fairness Reg Gap (train avg): 0.1600\n",
      "Temperature: 2.2736\n",
      "EarlyStopping score (normalized): 1.0000\n",
      "------------------------------------------------------------\n",
      "Early stopping patience: 5/5\n",
      "\n",
      "Early stopping triggered at epoch 7\n",
      "\n",
      "âœ“ Training complete!\n",
      "Best epoch: 2\n",
      "Best validation loss: 0.6888\n",
      "Best fairness score: 0.3158\n",
      "Training complete. Best epoch: 2\n",
      "Window-best epoch: None\n",
      "Early stopping patience: 5/5\n",
      "\n",
      "Early stopping triggered at epoch 7\n",
      "\n",
      "âœ“ Training complete!\n",
      "Best epoch: 2\n",
      "Best validation loss: 0.6888\n",
      "Best fairness score: 0.3158\n",
      "Training complete. Best epoch: 2\n",
      "Window-best epoch: None\n"
     ]
    }
   ],
   "source": [
    "# Train with window selection and balanced fairness\n",
    "history = trainer.train(\n",
    "    num_epochs=num_epochs,\n",
    "    early_stopping_patience=patience,\n",
    "    scheduler=scheduler,\n",
    "    warmup_steps=warmup_steps,\n",
    "    adv_lambda_target=ADVERSARIAL_LAMBDA,\n",
    "    multitask_lambda_target=MULTITASK_LAMBDA,\n",
    "    fairness_lambda_target=FAIRNESS_LAMBDA,\n",
    "    window_selection_start=WINDOW_SELECTION_START,\n",
    "    window_selection_end=WINDOW_SELECTION_END,\n",
    "    window_fairness_threshold=WINDOW_FAIRNESS_THRESHOLD\n",
    ")\n",
    "print(\"Training complete. Best epoch:\", history.get('best_epoch'))\n",
    "print(\"Window-best epoch:\", history.get('window_best_epoch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved to models/fair_adversarial\n"
     ]
    }
   ],
   "source": [
    "# Inspect metrics CSV and fairness trajectories\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "metrics_csv = Path('models/fair_adversarial/epoch_metrics.csv')\n",
    "if metrics_csv.exists():\n",
    "    df_metrics = pd.read_csv(metrics_csv)\n",
    "    display(df_metrics.tail())\n",
    "    print(\"Fairness raw (last 5):\", df_metrics['fairness_score_raw'].tail().tolist())\n",
    "    print(\"Fairness balanced (last 5):\", df_metrics['fairness_score_balanced'].tail().tolist())\n",
    "else:\n",
    "    print(\"Metrics CSV not found yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS matmul ok: torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test with best and window-best checkpoints\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "best_epoch = history.get('best_epoch')\n",
    "window_best_epoch = history.get('window_best_epoch')\n",
    "\n",
    "print(f\"Best epoch: {best_epoch}; Window-best epoch: {window_best_epoch}\")\n",
    "\n",
    "# Load window-best if exists\n",
    "window_ckpt = Path('models/fair_adversarial/window_best_model.pt')\n",
    "if window_ckpt.exists():\n",
    "    trainer.load_checkpoint(window_ckpt)\n",
    "    print(\"Loaded window-best checkpoint for test evaluation.\")\n",
    "else:\n",
    "    print(\"Window-best checkpoint not found; using current trainer state.\")\n",
    "\n",
    "# Simple test evaluation using stored best threshold\n",
    "best_thr_list = history.get('best_thresholds', [])\n",
    "if best_thr_list and best_epoch:\n",
    "    thr_idx = max(0, best_epoch-1)\n",
    "    best_thr = best_thr_list[thr_idx]\n",
    "else:\n",
    "    best_thr = 0.5\n",
    "\n",
    "model.eval()\n",
    "labels_all = []\n",
    "probs_all = []\n",
    "sc_all = []\n",
    "ts_all = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        ids = batch['input_ids'].to(DEVICE)\n",
    "        mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        out = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        logits = out['logits']\n",
    "        probs = torch.softmax(logits, dim=1)[:,1]\n",
    "        labels_all.extend(labels.cpu().numpy())\n",
    "        probs_all.extend(probs.cpu().numpy())\n",
    "        sc_all.extend(batch['school_category'].cpu().numpy())\n",
    "        ts_all.extend(batch['is_top_school'].cpu().numpy())\n",
    "\n",
    "import numpy as np\n",
    "labels_np = np.array(labels_all)\n",
    "probs_np = np.array(probs_all)\n",
    "preds_np = (probs_np >= best_thr).astype(int)\n",
    "acc_test = (preds_np == labels_np).mean() if labels_np.size else 0.0\n",
    "print(f\"Test accuracy (threshold={best_thr:.3f}): {acc_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>best_threshold</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>fairness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.691992</td>\n",
       "      <td>0.519708</td>\n",
       "      <td>0.006883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.690551</td>\n",
       "      <td>0.531387</td>\n",
       "      <td>0.006883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.687889</td>\n",
       "      <td>0.562044</td>\n",
       "      <td>0.006883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.686120</td>\n",
       "      <td>0.563504</td>\n",
       "      <td>0.006883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.684175</td>\n",
       "      <td>0.556204</td>\n",
       "      <td>0.006883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  best_threshold  val_loss   val_acc  fairness_score\n",
       "0      1            0.05  0.691992  0.519708        0.006883\n",
       "1      2            0.05  0.690551  0.531387        0.006883\n",
       "2      3            0.05  0.687889  0.562044        0.006883\n",
       "3      4            0.05  0.686120  0.563504        0.006883\n",
       "4      5            0.05  0.684175  0.556204        0.006883"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 5\n",
      "Best epoch threshold: 0.05\n"
     ]
    }
   ],
   "source": [
    "# Summary of training history\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"History keys:\", list(history.keys()))\n",
    "print(\"Best epoch:\", history.get('best_epoch'))\n",
    "print(\"Window-best epoch:\", history.get('window_best_epoch'))\n",
    "print(\"Temperature trajectory:\", history.get('temperature')[-5:])\n",
    "print(\"Adv lambda final per epoch:\", history.get('adv_lambdas')[-5:])\n",
    "print(\"Multitask lambda final per epoch:\", history.get('multitask_lambdas')[-5:])\n",
    "print(\"Fairness reg gaps last 5:\", history.get('fairness_reg_gaps')[-5:])\n",
    "\n",
    "metrics_csv = Path('models/fair_adversarial/epoch_metrics.csv')\n",
    "if metrics_csv.exists():\n",
    "    df_metrics = pd.read_csv(metrics_csv)\n",
    "    print(\"\\nAUC last 5:\", df_metrics['val_auc'].tail().tolist())\n",
    "    print(\"PR-AUC last 5:\", df_metrics['val_pr_auc'].tail().tolist())\n",
    "else:\n",
    "    print(\"Metrics CSV not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
