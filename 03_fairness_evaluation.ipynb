{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Evaluation of the LoRA Fine-Tuned Model\n",
    "\n",
    "This notebook is dedicated to evaluating the fairness of the BAAI/bge-large-en-v1.5 model fine-tuned with LoRA techniques. We will analyze the model's performance across different educational background groups and compute various fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Dataset path: /Users/edwardhuang/Documents/GitHub/bge-lora-fairness-finetuning/data/processed/processed_resume_dataset_resplit\n",
      "\n",
      "üì¶ Loading dataset from /Users/edwardhuang/Documents/GitHub/bge-lora-fairness-finetuning/data/processed/processed_resume_dataset_resplit...\n",
      "   Train: 5,472 samples\n",
      "   Val:   685 samples\n",
      "   Test:  1,087 samples\n",
      "\n",
      "üìä School Distribution:\n",
      "\n",
      "   Train:\n",
      "     no_school_mentioned...........   884 (16.15%)\n",
      "     non_top_school................  4365 (79.77%)\n",
      "     top_school....................   223 ( 4.08%)\n",
      "\n",
      "   Val:\n",
      "     non_top_school................   547 (79.85%)\n",
      "     no_school_mentioned...........   111 (16.20%)\n",
      "     top_school....................    27 ( 3.94%)\n",
      "\n",
      "   Test:\n",
      "     non_top_school................   867 (79.76%)\n",
      "     no_school_mentioned...........   176 (16.19%)\n",
      "     top_school....................    44 ( 4.05%)\n",
      "\n",
      "üî§ Loading tokenizer: BAAI/bge-large-en-v1.5\n",
      "\n",
      "‚öñÔ∏è  Using weighted sampling for fairness...\n",
      "\n",
      "‚úÖ Data loaders created successfully!\n",
      "   Train batches: 342\n",
      "   Val batches:   43\n",
      "   Test batches:  68\n",
      "Test batches: 68\n",
      "\n",
      "‚öñÔ∏è  Using weighted sampling for fairness...\n",
      "\n",
      "‚úÖ Data loaders created successfully!\n",
      "   Train batches: 342\n",
      "   Val batches:   43\n",
      "   Test batches:  68\n",
      "Test batches: 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] school_category distribution: {0: 44, 1: 867, 2: 176}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] is_top_school distribution: {0: 1043, 1: 44}\n",
      "üîß Loading base model: BAAI/bge-large-en-v1.5\n",
      "üîß Applying LoRA (r=8, alpha=16)...\n",
      "trainable params: 1,179,648 || all params: 336,321,536 || trainable%: 0.3508\n",
      "üîß Adding adversarial discriminator...\n",
      "üîß Adding attribute classifier for multi-task learning...\n",
      "Loading checkpoint: models/fair_adversarial/best_model.pt\n",
      "üîß Applying LoRA (r=8, alpha=16)...\n",
      "trainable params: 1,179,648 || all params: 336,321,536 || trainable%: 0.3508\n",
      "üîß Adding adversarial discriminator...\n",
      "üîß Adding attribute classifier for multi-task learning...\n",
      "Loading checkpoint: models/fair_adversarial/best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 1087\n"
     ]
    }
   ],
   "source": [
    "# Fairness evaluation pipeline (updated to match FairLoRA + fair_data_loader)\n",
    "import os\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"1\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from fair_lora_config import DEVICE, DATASET_PATH, BASE_MODEL, SCHOOL_CATEGORIES\n",
    "from src.fair_data_loader import create_data_loaders\n",
    "from src.fair_lora_model import FairLoRAModel\n",
    "from src.fairness_metrics import FairnessMetrics, compute_fairness_report\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "\n",
    "# 1) Load test set\n",
    "_, _, test_loader, tokenizer = create_data_loaders(dataset_path=str(DATASET_PATH))\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Quick distribution check for test\n",
    "def group_dist(loader, key):\n",
    "    vals = []\n",
    "    for batch in loader:\n",
    "        vals.append(batch[key].cpu().numpy())\n",
    "    if len(vals) == 0:\n",
    "        return {}\n",
    "    v = np.concatenate(vals)\n",
    "    u, c = np.unique(v, return_counts=True)\n",
    "    return {int(k): int(v) for k, v in zip(u.tolist(), c.tolist())}\n",
    "print(\"[Test] school_category distribution:\", group_dist(test_loader, 'school_category'))\n",
    "print(\"[Test] is_top_school distribution:\", group_dist(test_loader, 'is_top_school'))\n",
    "\n",
    "# 2) Load model and best checkpoint\n",
    "model = FairLoRAModel(base_model_name=BASE_MODEL, use_lora=True, use_adversarial=True, use_multitask=True, num_labels=2)\n",
    "ckpt_dir = Path(\"models/fair_adversarial\")\n",
    "best_ckpt = ckpt_dir / \"best_model.pt\"\n",
    "if not best_ckpt.exists():\n",
    "    # fallback to last checkpoint by epoch if present\n",
    "    candidates = sorted(ckpt_dir.glob(\"checkpoint_epoch_*.pt\"))\n",
    "    if len(candidates) == 0:\n",
    "        raise FileNotFoundError(f\"No checkpoint found in {ckpt_dir}\")\n",
    "    best_ckpt = candidates[-1]\n",
    "print(\"Loading checkpoint:\", best_ckpt)\n",
    "state = torch.load(best_ckpt, map_location=DEVICE)\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# 3) Inference on test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_prob = []\n",
    "g_is_top = []\n",
    "g_school_cat = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs['logits']\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_prob.extend(probs.cpu().numpy())\n",
    "        g_is_top.extend(batch['is_top_school'].cpu().numpy())\n",
    "        g_school_cat.extend(batch['school_category'].cpu().numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_prob = np.array(y_prob)\n",
    "g_is_top = np.array(g_is_top)\n",
    "g_school_cat = np.array(g_school_cat)\n",
    "\n",
    "print(f\"Test size: {len(y_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc': 0.4640748564619217, 'pr_auc': 0.46893999764178007}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3.2) Probability distribution and curves (ROC AUC, PR AUC)\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(np.unique(y_true)) < 2:\n",
    "    print('[Info] Test labels have a single class. ROC/PR AUC not defined.')\n",
    "else:\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, y_prob)\n",
    "        pr_auc = average_precision_score(y_true, y_prob)\n",
    "        print({'roc_auc': float(roc_auc), 'pr_auc': float(pr_auc)})\n",
    "    except Exception as e:\n",
    "        print('AUC computation error:', e)\n",
    "\n",
    "# Histogram of predicted probabilities (overall and by true class)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(y_prob, bins=30, alpha=0.6, label='all', color='#4C78A8')\n",
    "if len(y_true) == len(y_prob):\n",
    "    plt.hist(y_prob[y_true==1], bins=30, alpha=0.6, label='true=1', color='#F58518')\n",
    "    plt.hist(y_prob[y_true==0], bins=30, alpha=0.6, label='true=0', color='#54A24B')\n",
    "plt.title('Predicted probability distribution')\n",
    "plt.xlabel('P(y=1)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6) Inspect train/val label distribution to diagnose absence of positives in test\n",
    "from collections import Counter\n",
    "train_loader, val_loader, _, _ = create_data_loaders(dataset_path=str(DATASET_PATH))\n",
    "def loader_label_dist(loader):\n",
    "    pos=0; neg=0\n",
    "    for batch in loader:\n",
    "        labels = batch['label']\n",
    "        pos += (labels==1).sum().item()\n",
    "        neg += (labels==0).sum().item()\n",
    "    return {'pos':pos,'neg':neg,'total':pos+neg,'pos_rate': (pos/(pos+neg)) if (pos+neg)>0 else 0}\n",
    "print('[Train label dist]', loader_label_dist(train_loader))\n",
    "print('[Val label dist]', loader_label_dist(val_loader))\n",
    "print('[Test label dist] (already above)', {'pos':int((y_true==1).sum()),'neg':int((y_true==0).sum()),'total':int(len(y_true)), 'pos_rate': float((y_true==1).mean())})\n",
    "print('\\nSample train items (first 3) label raw + mapped:')\n",
    "raw_train = load_from_disk(str(DATASET_PATH))['train']\n",
    "for i in range(min(3,len(raw_train))):\n",
    "    raw_label = raw_train[i]['label']\n",
    "    mapped = 1 if raw_label.lower()=='fit' else 0\n",
    "    print(f\"idx={i} raw_label={raw_label} mapped={mapped} is_top={raw_train[i]['is_top_school']} school_cat={raw_train[i]['school_category']}\")\n",
    "if (y_true==1).sum()==0:\n",
    "    print('\\n[Action Needed] No positives in test split. Consider:')\n",
    "    print(' - Re-check original CSV for label column correctness (maybe all \"No Fit\" in test).')\n",
    "    print(' - Re-run split ensuring stratification by label to keep positives in test.')\n",
    "    print(' - If positives are extremely rare globally, merge part of train into test or perform cross-validation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Diagnostics] Overall distribution and sanity checks\n",
      "{'n_samples': 1759, 'pos_count': 0, 'neg_count': 1759, 'pos_rate_true': 0.0}\n",
      "{'pred_pos_rate': 0.0, 'pred_neg_rate': 1.0}\n",
      "Confusion matrix [rows=true 0/1, cols=pred 0/1]:\n",
      "[[1759    0]\n",
      " [   0    0]]\n",
      "school_category ‚Äî true positives per group: {}\n",
      "school_category ‚Äî predicted positives per group: {}\n",
      "is_top_school ‚Äî true positives per group: {}\n",
      "is_top_school ‚Äî predicted positives per group: {}\n",
      "[Warning] No positive labels in test set. Many fairness metrics become uninformative (TPR=0, DIR=1 by 0/0).\n",
      "[Warning] Model predicted no positives. Consider threshold tuning, calibration, or checking label mapping.\n"
     ]
    }
   ],
   "source": [
    "# 3.5) Diagnostics: label/pred distributions and confusion matrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"\\n[Diagnostics] Overall distribution and sanity checks\")\n",
    "print({\n",
    "    'n_samples': int(len(y_true)),\n",
    "    'pos_count': int((y_true==1).sum()),\n",
    "    'neg_count': int((y_true==0).sum()),\n",
    "    'pos_rate_true': float((y_true==1).mean())\n",
    "})\n",
    "print({'pred_pos_rate': float((y_pred==1).mean()), 'pred_neg_rate': float((y_pred==0).mean())})\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "print('Confusion matrix [rows=true 0/1, cols=pred 0/1]:')\n",
    "print(cm)\n",
    "\n",
    "def group_pos_counts(sensitive, name):\n",
    "    import numpy as np\n",
    "    u, c_true_pos = np.unique(sensitive[y_true==1], return_counts=True)\n",
    "    u2, c_pred_pos = np.unique(sensitive[y_pred==1], return_counts=True)\n",
    "    d_true = {int(k): int(v) for k,v in zip(u.tolist(), c_true_pos.tolist())}\n",
    "    d_pred = {int(k): int(v) for k,v in zip(u2.tolist(), c_pred_pos.tolist())}\n",
    "    print(f\"{name} ‚Äî true positives per group:\", d_true)\n",
    "    print(f\"{name} ‚Äî predicted positives per group:\", d_pred)\n",
    "    return d_true, d_pred\n",
    "\n",
    "group_pos_counts(g_school_cat, 'school_category')\n",
    "group_pos_counts(g_is_top, 'is_top_school')\n",
    "\n",
    "# Warn if there are no true positives or no predicted positives (common cause of 0/1 fairness metrics)\n",
    "if (y_true==1).sum() == 0:\n",
    "    print('[Warning] No positive labels in test set. Many fairness metrics become uninformative (TPR=0, DIR=1 by 0/0).')\n",
    "if (y_pred==1).sum() == 0:\n",
    "    print('[Warning] Model predicted no positives. Consider threshold tuning, calibration, or checking label mapping.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fairness evaluation to reports/fairness_metrics/fairness_evaluation.json\n",
      "\n",
      "=== school_category ===\n",
      "Demographic Parity Diff: 0.0000\n",
      "Equalized Odds Avg Diff: 0.0000\n",
      "Equal Opportunity Diff: 0.0000\n",
      "Disparate Impact Ratio: 1.0000\n",
      "Group Performance:\n",
      "  Group 0: count=73 acc=1.000 recall=0.000 f1=0.000\n",
      "  Group 1: count=1422 acc=1.000 recall=0.000 f1=0.000\n",
      "  Group 2: count=264 acc=1.000 recall=0.000 f1=0.000\n",
      "\n",
      "=== is_top_school ===\n",
      "Demographic Parity Diff: 0.0000\n",
      "Equalized Odds Avg Diff: 0.0000\n",
      "Equal Opportunity Diff: 0.0000\n",
      "Disparate Impact Ratio: 1.0000\n",
      "Group Performance:\n",
      "  Group 0: count=1686 acc=1.000 recall=0.000 f1=0.000\n",
      "  Group 1: count=73 acc=1.000 recall=0.000 f1=0.000\n"
     ]
    }
   ],
   "source": [
    "# 4) Fairness metrics (Statistical Parity, Equalized Odds, Disparate Impact)\n",
    "metrics_calc = FairnessMetrics()\n",
    "\n",
    "# Helper: mark value as None/np.nan if metrics invalid\n",
    "def _safe(v):\n",
    "    return float(v) if v is not None and not (isinstance(v, float) and np.isnan(v)) else None\n",
    "\n",
    "def disparate_impact_ratio(y_pred, sensitive):\n",
    "    # Ratio of positive prediction rates min/max (four-fifths rule)\n",
    "    unique = np.unique(sensitive)\n",
    "    rates = []\n",
    "    for g in unique:\n",
    "        mask = sensitive == g\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        rates.append(y_pred[mask].mean())\n",
    "    if len(rates) < 2:\n",
    "        return np.nan\n",
    "    rates = np.array(rates)\n",
    "    safe = rates + 1e-8\n",
    "    return safe.min() / safe.max()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "def group_perf(y_true, y_pred, sensitive):\n",
    "    out = {}\n",
    "    for g in np.unique(sensitive):\n",
    "        mask = sensitive == g\n",
    "        yt = y_true[mask]\n",
    "        yp = y_pred[mask]\n",
    "        if len(yt) == 0:\n",
    "            continue\n",
    "        out[int(g)] = {\n",
    "            'count': int(len(yt)),\n",
    "            'accuracy': float(accuracy_score(yt, yp)),\n",
    "            'precision': float(precision_score(yt, yp, zero_division=0)),\n",
    "            'recall': float(recall_score(yt, yp, zero_division=0)),\n",
    "            'f1': float(f1_score(yt, yp, zero_division=0)),\n",
    "        }\n",
    "    return out\n",
    "\n",
    "# Detect degenerate prediction scenario (all negatives)\n",
    "all_neg_preds = (y_pred == 0).all()\n",
    "no_true_pos = (y_true == 1).sum() == 0\n",
    "fair_sc = metrics_calc.compute_all_metrics(y_true, y_pred, y_prob, g_school_cat, 'school_category') if not all_neg_preds else {}\n",
    "fair_top = metrics_calc.compute_all_metrics(y_true, y_pred, y_prob, g_is_top, 'is_top_school') if not all_neg_preds else {}\n",
    "\n",
    "di_school = disparate_impact_ratio(y_pred, g_school_cat) if not all_neg_preds else np.nan\n",
    "di_top = disparate_impact_ratio(y_pred, g_is_top) if not all_neg_preds else np.nan\n",
    "\n",
    "perf_school = group_perf(y_true, y_pred, g_school_cat)\n",
    "perf_top = group_perf(y_true, y_pred, g_is_top)\n",
    "\n",
    "summary = {\n",
    "    'degenerate': {\n",
    "        'all_negative_predictions': bool(all_neg_preds),\n",
    "        'no_true_positive_labels': bool(no_true_pos)\n",
    "    },\n",
    "    'school_category': {\n",
    "        'fairness_metrics': fair_sc,\n",
    "        'disparate_impact_ratio': di_school,\n",
    "        'group_performance': perf_school,\n",
    "    },\n",
    "    'is_top_school': {\n",
    "        'fairness_metrics': fair_top,\n",
    "        'disparate_impact_ratio': di_top,\n",
    "        'group_performance': perf_top,\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "out_dir = Path('reports/metrics')\n",
    "if out_dir.exists() and not out_dir.is_dir():\n",
    "    alt_dir = Path('reports/fairness_metrics')\n",
    "    alt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_dir = alt_dir\n",
    "else:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(out_dir / 'fairness_evaluation.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f'Saved fairness evaluation to {out_dir / \"fairness_evaluation.json\"}')\n",
    "\n",
    "# Display key metrics succinctly\n",
    "def display_core(attr_name, data):\n",
    "    fm = data['fairness_metrics']\n",
    "    print(f\"\\n=== {attr_name} ===\")\n",
    "    if not fm:\n",
    "        print('[Skipped] Degenerate predictions (all negatives); fairness metrics not informative.')\n",
    "        return\n",
    "    def fmt(key):\n",
    "        val = fm.get(key)\n",
    "        return f\"{val:.4f}\" if isinstance(val, (int,float)) else 'N/A'\n",
    "    print(f\"Demographic Parity Diff: {fmt('demographic_parity_difference')}\")\n",
    "    print(f\"Equalized Odds Avg Diff: {fmt('equalized_odds_avg_difference')}\")\n",
    "    print(f\"Equal Opportunity Diff: {fmt('equal_opportunity_difference')}\")\n",
    "    dir_val = data['disparate_impact_ratio']\n",
    "    print(f\"Disparate Impact Ratio: {dir_val:.4f}\" if isinstance(dir_val,(int,float)) and not np.isnan(dir_val) else \"Disparate Impact Ratio: N/A\")\n",
    "    print(\"Group Performance:\")\n",
    "    for g, stats in data['group_performance'].items():\n",
    "        print(f\"  Group {g}: count={stats['count']} acc={stats['accuracy']:.3f} recall={stats['recall']:.3f} f1={stats['f1']:.3f}\")\n",
    "\n",
    "print('\\nDegenerate flags:', summary['degenerate'])\n",
    "display_core('school_category', summary['school_category'])\n",
    "display_core('is_top_school', summary['is_top_school'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'split': 'train', 'total': 5472, 'fit_count': 0, 'no_fit_count': 5472, 'fit_rate': 0.0}\n",
      "{'split': 'test', 'total': 1087, 'fit_count': 0, 'no_fit_count': 1087, 'fit_rate': 0.0}\n",
      "Unique school_category train: {'no_school_mentioned', 'top_school', 'non_top_school'}\n",
      "Unique school_category test: {'no_school_mentioned', 'top_school', 'non_top_school'}\n"
     ]
    }
   ],
   "source": [
    "# 5) Raw label distribution (current processed dataset)\n",
    "from datasets import load_from_disk\n",
    "ds = load_from_disk(str(DATASET_PATH))\n",
    "train_ds = ds['train']\n",
    "test_ds = ds['test']\n",
    "def label_stats(dset,name):\n",
    "    import numpy as np\n",
    "    labels = [x.lower() if isinstance(x,str) else str(x).lower() for x in dset['label']]\n",
    "    fit = sum(1 for x in labels if x in {'fit','1','true','yes'})\n",
    "    total = len(labels)\n",
    "    return {'split':name,'total':total,'fit_count':fit,'no_fit_count':total-fit,'fit_rate': fit/total if total else 0}\n",
    "print(label_stats(train_ds,'train'))\n",
    "print(label_stats(test_ds,'test'))\n",
    "print(\"Unique school_category train:\", set(train_ds['school_category']))\n",
    "print(\"Unique school_category test:\", set(test_ds['school_category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Raw label values] {'no fit': 3622, 'potential fit': 1824, 'good fit': 1798}\n",
      "{'total_samples': 7244, 'total_pos': 3622, 'pos_rate': 0.5}\n",
      "Strata counts after rare merge: {'1_non_top_school': 2903, '0_non_top_school': 2876, '0_no_school_mentioned': 594, '1_no_school_mentioned': 577, '0_top_school': 152, '1_top_school': 142}\n",
      "[Split OK] seed=42 pos_train/val/test= 2735 343 544\n",
      "{'train_new': {'n': 5472, 'fit': 2735, 'fit_rate': 0.4998172514619883}}\n",
      "{'val_new': {'n': 685, 'fit': 343, 'fit_rate': 0.5007299270072992}}\n",
      "{'test_new': {'n': 1087, 'fit': 544, 'fit_rate': 0.500459981600736}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4127401abe824c50ba6badf4ea798d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd792d5c2d944d0a41d50d9e947b020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/685 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae59d5bdcf741c895d360b1a93bf409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1087 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved re-split dataset to data/processed/processed_resume_dataset_resplit\n"
     ]
    }
   ],
   "source": [
    "# 6) Re-splitting with combined stratification (label + school_category) with robust label parsing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all_records = []\n",
    "for split_name, split_ds in [('train', train_ds), ('test', test_ds)]:\n",
    "    for i in range(len(split_ds)):\n",
    "        all_records.append({\n",
    "            'resume_text': split_ds[i]['resume_text'],\n",
    "            'job_description_text': split_ds[i]['job_description_text'],\n",
    "            'label': split_ds[i]['label'],\n",
    "            'is_top_school': split_ds[i]['is_top_school'],\n",
    "            'school_category': split_ds[i]['school_category']\n",
    "        })\n",
    "\n",
    "df_all = pd.DataFrame(all_records)\n",
    "\n",
    "# Robust label binarization (trim + synonyms + numeric/boolean)\n",
    "POS_STRINGS = {\n",
    "    'fit','good fit','potential fit','1','true','yes','y','positive','pos','match'\n",
    "}\n",
    "NEG_STRINGS = {\n",
    "    'not fit','no fit','0','false','no','n','negative','neg','mismatch'\n",
    "}\n",
    "\n",
    "def to_bin(v):\n",
    "    try:\n",
    "        # Strings\n",
    "        if isinstance(v, str):\n",
    "            s = v.strip().lower()\n",
    "            if s in POS_STRINGS:\n",
    "                return 1\n",
    "            if s in NEG_STRINGS:\n",
    "                return 0\n",
    "            try:\n",
    "                return int(float(s))\n",
    "            except Exception:\n",
    "                return 0\n",
    "        # Booleans\n",
    "        if isinstance(v, (bool, np.bool_)):\n",
    "            return int(v)\n",
    "        # Integers\n",
    "        if isinstance(v, (int, np.integer)):\n",
    "            return 1 if int(v) >= 1 else 0\n",
    "        # Floats\n",
    "        if isinstance(v, (float, np.floating)):\n",
    "            return 1 if float(v) > 0.5 else 0\n",
    "    except Exception:\n",
    "        pass\n",
    "    return 0\n",
    "\n",
    "# Show raw label summary before binning\n",
    "raw_counts = df_all['label'].astype(str).str.strip().str.lower().value_counts().to_dict()\n",
    "print('[Raw label values]', raw_counts)\n",
    "\n",
    "# Apply robust binarization\n",
    "df_all['label_bin'] = df_all['label'].apply(to_bin).clip(0,1).astype(int)\n",
    "\n",
    "# Guard: if no positives globally, abort saving\n",
    "total_pos = int(df_all['label_bin'].sum())\n",
    "print({'total_samples': len(df_all), 'total_pos': total_pos, 'pos_rate': float(df_all['label_bin'].mean())})\n",
    "if total_pos == 0:\n",
    "    print('[Abort] No positive labels found globally after robust parsing. Please inspect source labels.')\n",
    "else:\n",
    "    # Stratification key (merge label + school_category)\n",
    "    df_all['strat_key'] = df_all['label_bin'].astype(str) + '_' + df_all['school_category'].astype(str)\n",
    "    # Merge rare strata with <2 samples into label-only stratum\n",
    "    strata_counts = df_all['strat_key'].value_counts()\n",
    "    rare = strata_counts[strata_counts < 2].index.tolist()\n",
    "    if rare:\n",
    "        df_all.loc[df_all['strat_key'].isin(rare),'strat_key'] = df_all.loc[df_all['strat_key'].isin(rare),'label_bin'].astype(str)\n",
    "    print('Strata counts after rare merge:', df_all['strat_key'].value_counts().to_dict())\n",
    "\n",
    "    # Try multiple random seeds to ensure each split keeps positives\n",
    "    def make_splits(seed):\n",
    "        sss_outer = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=seed)\n",
    "        idx_all = df_all.index.values\n",
    "        y_strat = df_all['strat_key'].values\n",
    "        train_val_idx, test_new_idx = next(sss_outer.split(idx_all, y_strat))\n",
    "        df_tv = df_all.loc[train_val_idx].reset_index(drop=True)\n",
    "        df_test_new = df_all.loc[test_new_idx].reset_index(drop=True)\n",
    "\n",
    "        sss_inner = StratifiedShuffleSplit(n_splits=1, test_size=0.1111, random_state=seed)  # ~0.1 of total for val\n",
    "        idx_tv = df_tv.index.values\n",
    "        y_tv = df_tv['strat_key'].values\n",
    "        train_idx_new, val_idx_new = next(sss_inner.split(idx_tv, y_tv))\n",
    "        df_train_new = df_tv.loc[train_idx_new].reset_index(drop=True)\n",
    "        df_val_new = df_tv.loc[val_idx_new].reset_index(drop=True)\n",
    "        return df_train_new, df_val_new, df_test_new\n",
    "\n",
    "    ok = False\n",
    "    for trial_seed in [42, 7, 2024, 123, 999]:\n",
    "        df_train_new, df_val_new, df_test_new = make_splits(trial_seed)\n",
    "        pos_train = int(df_train_new['label_bin'].sum())\n",
    "        pos_val = int(df_val_new['label_bin'].sum())\n",
    "        pos_test = int(df_test_new['label_bin'].sum())\n",
    "        if pos_train > 0 and pos_val > 0 and pos_test > 0:\n",
    "            ok = True\n",
    "            print(f'[Split OK] seed={trial_seed} pos_train/val/test=', pos_train, pos_val, pos_test)\n",
    "            break\n",
    "        else:\n",
    "            print(f'[Split Retry] seed={trial_seed} pos_train/val/test=', pos_train, pos_val, pos_test)\n",
    "\n",
    "    if not ok:\n",
    "        print('[Warning] Could not ensure positives in every split. Proceeding with last attempt but fairness metrics may be uninformative.')\n",
    "\n",
    "    def report(df,name):\n",
    "        return {name:{'n':len(df),'fit':int(df['label_bin'].sum()),'fit_rate':float(df['label_bin'].mean())}}\n",
    "    print(report(df_train_new,'train_new'))\n",
    "    print(report(df_val_new,'val_new'))\n",
    "    print(report(df_test_new,'test_new'))\n",
    "\n",
    "    # Convert back to HF Dataset (preserve original label field as-is; FairResumeDataset will robustly map)\n",
    "    from datasets import Dataset, DatasetDict\n",
    "    def to_dataset(df):\n",
    "        return Dataset.from_pandas(df[['resume_text','job_description_text','label','is_top_school','school_category']])\n",
    "    new_ds_dict = DatasetDict({'train': to_dataset(df_train_new), 'validation': to_dataset(df_val_new), 'test': to_dataset(df_test_new)})\n",
    "    resplit_path = Path('data/processed/processed_resume_dataset_resplit')\n",
    "    new_ds_dict.save_to_disk(str(resplit_path))\n",
    "    print('Saved re-split dataset to', resplit_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_size': 685, 'val_pos': 343, 'val_neg': 342}\n",
      "{'best_threshold': 0.08, 'best_score': 0.564187186587424}\n",
      "{'test_size': 1087, 'test_pos': 544, 'pred_pos_rate': 0.7552897884084636}\n",
      "{'test_size': 1087, 'test_pos': 544, 'pred_pos_rate': 0.7552897884084636}\n",
      "\n",
      "[Fairness @ best threshold]\n",
      "school_category DP Diff: 0.4772727272727273\n",
      "school_category EO Avg : 0.4778509617631602\n",
      "is_top_school DP Diff  : 0.25503355704697983\n",
      "is_top_school EO Avg   : 0.2549805113987771\n",
      "\n",
      "[Fairness @ best threshold]\n",
      "school_category DP Diff: 0.4772727272727273\n",
      "school_category EO Avg : 0.4778509617631602\n",
      "is_top_school DP Diff  : 0.25503355704697983\n",
      "is_top_school EO Avg   : 0.2549805113987771\n"
     ]
    }
   ],
   "source": [
    "# 7) Enhanced Threshold search with constraints and calibration penalty\n",
    "from datasets import load_from_disk\n",
    "import importlib, src.fair_data_loader as fdl\n",
    "importlib.reload(fdl)\n",
    "from src.fair_data_loader import FairResumeDataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Objective weights\n",
    "CALIBRATION_TARGET = 0.5           # desired predicted positive rate ~ prevalence\n",
    "CALIBRATION_WEIGHT = 0.2           # penalty weight for deviation from prevalence\n",
    "POS_RATE_MIN, POS_RATE_MAX = 0.1, 0.9  # allowable predicted positive rate window\n",
    "\n",
    "# Load sets\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "new_ds = load_from_disk('data/processed/processed_resume_dataset_resplit')\n",
    "val_set = FairResumeDataset(new_ds['validation'], tok)\n",
    "test_set = FairResumeDataset(new_ds['test'], tok)\n",
    "val_loader_new = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "test_loader_new = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "def collect_probs(loader):\n",
    "    probs=[]; labels=[]\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            ids = b['input_ids'].to(DEVICE)\n",
    "            mask = b['attention_mask'].to(DEVICE)\n",
    "            lab = b['label'].to(DEVICE)\n",
    "            out = model(input_ids=ids, attention_mask=mask, labels=lab)\n",
    "            logits = out['logits']\n",
    "            pr = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
    "            probs.append(pr)\n",
    "            labels.append(lab.cpu().numpy())\n",
    "    return (np.concatenate(probs) if probs else np.array([]),\n",
    "            np.concatenate(labels) if labels else np.array([]))\n",
    "\n",
    "val_probs, val_labels = collect_probs(val_loader_new)\n",
    "print({'val_size': len(val_labels), 'val_pos': int((val_labels==1).sum()), 'val_neg': int((val_labels==0).sum())})\n",
    "\n",
    "if (val_labels==1).sum()==0 or (val_labels==0).sum()==0:\n",
    "    print('[Warning] Degenerate validation (single class). Using default threshold=0.5')\n",
    "    best_thr = 0.5\n",
    "else:\n",
    "    thr_grid = np.linspace(0.01, 0.99, 99)\n",
    "    best_score = -1e9\n",
    "    best_thr = 0.5\n",
    "    prevalence = (val_labels==1).mean()\n",
    "    for thr in thr_grid:\n",
    "        preds = (val_probs >= thr).astype(int)\n",
    "        pos_rate = preds.mean()\n",
    "        # constraint: pos rate window\n",
    "        if pos_rate < POS_RATE_MIN or pos_rate > POS_RATE_MAX:\n",
    "            continue\n",
    "        f1 = f1_score(val_labels, preds, zero_division=0)\n",
    "        # Youden's J\n",
    "        cm = confusion_matrix(val_labels, preds, labels=[0,1])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        tpr = tp / (tp + fn) if (tp+fn)>0 else 0\n",
    "        tnr = tn / (tn + fp) if (tn+fp)>0 else 0\n",
    "        youden_j = tpr + tnr - 1\n",
    "        # Calibration penalty: absolute deviation from prevalence\n",
    "        calib_pen = abs(pos_rate - prevalence)\n",
    "        score = f1 + 0.3*youden_j - CALIBRATION_WEIGHT*calib_pen\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thr = float(thr)\n",
    "    print({'best_threshold': best_thr, 'best_score': best_score})\n",
    "\n",
    "# Evaluate on test\n",
    "from src.fairness_metrics import FairnessMetrics\n",
    "metrics_calc = FairnessMetrics()\n",
    "\n",
    "test_probs, test_labels = collect_probs(test_loader_new)\n",
    "test_preds = (test_probs >= best_thr).astype(int)\n",
    "print({'test_size': len(test_labels), 'test_pos': int((test_labels==1).sum()), 'pred_pos_rate': float(test_preds.mean())})\n",
    "\n",
    "# Fairness (non-degenerate only)\n",
    "if (test_labels==1).sum()>0 and (test_labels==0).sum()>0 and test_preds.mean()>0 and test_preds.mean()<1:\n",
    "    # Need sensitive attributes; reload batches\n",
    "    g_top=[]; g_school=[]\n",
    "    with torch.no_grad():\n",
    "        for b in test_loader_new:\n",
    "            g_top.append(b['is_top_school'].cpu().numpy())\n",
    "            g_school.append(b['school_category'].cpu().numpy())\n",
    "    g_top = np.concatenate(g_top)\n",
    "    g_school = np.concatenate(g_school)\n",
    "    fair_sc_new = metrics_calc.compute_all_metrics(test_labels, test_preds, test_probs, g_school, 'school_category')\n",
    "    fair_top_new = metrics_calc.compute_all_metrics(test_labels, test_preds, test_probs, g_top, 'is_top_school')\n",
    "    print('\\n[Fairness @ best threshold]')\n",
    "    print('school_category DP Diff:', fair_sc_new.get('demographic_parity_difference'))\n",
    "    print('school_category EO Avg :', fair_sc_new.get('equalized_odds_avg_difference'))\n",
    "    print('is_top_school DP Diff  :', fair_top_new.get('demographic_parity_difference'))\n",
    "    print('is_top_school EO Avg   :', fair_top_new.get('equalized_odds_avg_difference'))\n",
    "else:\n",
    "    print('[Skipped] Fairness (degenerate predictions or labels).')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
