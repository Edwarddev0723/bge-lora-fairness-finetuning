{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fdaf026",
   "metadata": {},
   "source": [
    "# Publish Fairness LoRA Adapter to Hugging Face\n",
    "\n",
    "This notebook exports the fairness-aware LoRA adapter from the trained checkpoint, writes a Model Card (README.md), and optionally pushes the adapter folder to the Hugging Face Hub.\n",
    "\n",
    "Steps:\n",
    "- Build the fair LoRA model and load the checkpoint.\n",
    "- Save only the PEFT adapter to `models/lora_adapters/fairness_lora`.\n",
    "- Auto-generate a Model Card with metrics (if available).\n",
    "- Optionally push the adapter repo to the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9742af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Imports and project path setup\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Resolve project root robustly (handles notebook working dir variance)\n",
    "nb_dir = Path.cwd()\n",
    "# Candidate roots to add: current dir, parent, and repo root if notebooks/ layout\n",
    "candidates = [\n",
    "    nb_dir,\n",
    "    nb_dir.parent,\n",
    "    nb_dir.parent.parent,\n",
    "]\n",
    "for c in candidates:\n",
    "    if str(c) not in sys.path:\n",
    "        sys.path.insert(0, str(c))\n",
    "\n",
    "# Import helper functions from the export script\n",
    "try:\n",
    "    from scripts.export_fair_lora_to_hf import (\n",
    "        build_model,\n",
    "        load_checkpoint_into_model,\n",
    "        maybe_collect_metrics,\n",
    "        write_model_card,\n",
    "        push_to_hub,\n",
    "    )\n",
    "except ModuleNotFoundError as e:\n",
    "    # As a fallback, try to import by absolute path based on repo structure\n",
    "    repo_root = None\n",
    "    for c in candidates:\n",
    "        if (c / 'scripts' / 'export_fair_lora_to_hf.py').exists():\n",
    "            repo_root = c\n",
    "            break\n",
    "    if repo_root and str(repo_root) not in sys.path:\n",
    "        sys.path.insert(0, str(repo_root))\n",
    "    from scripts.export_fair_lora_to_hf import (\n",
    "        build_model,\n",
    "        load_checkpoint_into_model,\n",
    "        maybe_collect_metrics,\n",
    "        write_model_card,\n",
    "        push_to_hub,\n",
    "    )\n",
    "\n",
    "print('Torch version:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7be17525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: BAAI/bge-large-en-v1.5\n",
      "Checkpoint: /Users/edwardhuang/Documents/GitHub/models/fair_adversarial/best_fairness_model.pt\n",
      "Adapter out dir: /Users/edwardhuang/Documents/GitHub/models/peft_adapters/fairness_lora\n",
      "Repo id (optional): renhehuang/fair-resume-job-matcher-lora\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "try:\n",
    "    from fair_lora_config import BASE_MODEL as CONFIG_BASE_MODEL\n",
    "except Exception:\n",
    "    CONFIG_BASE_MODEL = 'BAAI/bge-large-en-v1.5'\n",
    "\n",
    "BASE_MODEL = CONFIG_BASE_MODEL  # Base encoder model on which the LoRA adapter sits\n",
    "CHECKPOINT = '../models/fair_adversarial/best_fairness_model.pt'  # Trained fairness model checkpoint\n",
    "ADAPTER_DIR = '../models/peft_adapters/fairness_lora'  # Where to save the adapter locally (avoid clashing with any file)\n",
    "REPO_ID = 'renhehuang/fair-resume-job-matcher-lora'  # e.g., 'your-username/fair-resume-matcher-lora' when pushing to Hub\n",
    "PRIVATE = False  # If pushing, create a private repo if True\n",
    "DO_PUSH = True  # Set True to push to Hub (requires REPO_ID)\n",
    "\n",
    "# Resolve repo root and make paths absolute for robustness\n",
    "_nb_dir = Path.cwd()\n",
    "_candidates = [_nb_dir, _nb_dir.parent, _nb_dir.parent.parent]\n",
    "_repo_root = None\n",
    "for c in _candidates:\n",
    "    if (c / 'scripts' / 'export_fair_lora_to_hf.py').exists():\n",
    "        _repo_root = c\n",
    "        break\n",
    "_repo_root = _repo_root or _nb_dir\n",
    "\n",
    "ckpt_path = Path(CHECKPOINT)\n",
    "if not ckpt_path.is_absolute():\n",
    "    ckpt_path = (_repo_root / ckpt_path).resolve()\n",
    "adapter_dir = Path(ADAPTER_DIR)\n",
    "if not adapter_dir.is_absolute():\n",
    "    adapter_dir = (_repo_root / adapter_dir).resolve()\n",
    "\n",
    "print('Base model:', BASE_MODEL)\n",
    "print('Checkpoint:', ckpt_path)\n",
    "print('Adapter out dir:', adapter_dir)\n",
    "print('Repo id (optional):', REPO_ID or '(not set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c780b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Loading base model: BAAI/bge-large-en-v1.5\n",
      "ðŸ”§ Applying LoRA (expanded targets, r=16, alpha=32)...\n",
      "   â€¢ LoRA target modules detected: ['dense', 'key', 'query', 'value']\n",
      "     (modules not present are ignored silently by PEFT)\n",
      "ðŸ”§ Applying LoRA (expanded targets, r=16, alpha=32)...\n",
      "   â€¢ LoRA target modules detected: ['dense', 'key', 'query', 'value']\n",
      "     (modules not present are ignored silently by PEFT)\n",
      "ðŸ”’ Frozen encoder base weights except attention in last 4 layers; LoRA adapters remain trainable.\n",
      "ðŸ“Š Trainable params -> LoRA: 7,110,656 | Base last4(attn): 16,801,792 | Total: 23,912,448\n",
      "trainable params: 23,912,448 || all params: 342,252,544 || trainable%: 6.9868\n",
      "ðŸ”§ Adding adversarial discriminator...\n",
      "ðŸ”§ Adding attribute classifier for multi-task learning...\n",
      "ðŸ”’ Frozen encoder base weights except attention in last 4 layers; LoRA adapters remain trainable.\n",
      "ðŸ“Š Trainable params -> LoRA: 7,110,656 | Base last4(attn): 16,801,792 | Total: 23,912,448\n",
      "trainable params: 23,912,448 || all params: 342,252,544 || trainable%: 6.9868\n",
      "ðŸ”§ Adding adversarial discriminator...\n",
      "ðŸ”§ Adding attribute classifier for multi-task learning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint: /Users/edwardhuang/Documents/GitHub/bge-lora-fairness-finetuning/models/fair_adversarial/best_fairness_model.pt\n",
      "  Missing keys: 0 | Unexpected keys: 0\n",
      "âœ“ Saved PEFT adapter to: /Users/edwardhuang/Documents/GitHub/bge-lora-fairness-finetuning/models/peft_adapters/fairness_lora\n",
      "Saved files:\n",
      " - README.md\n",
      " - adapter_config.json\n",
      " - adapter_model.safetensors\n",
      "âœ“ Saved PEFT adapter to: /Users/edwardhuang/Documents/GitHub/bge-lora-fairness-finetuning/models/peft_adapters/fairness_lora\n",
      "Saved files:\n",
      " - README.md\n",
      " - adapter_config.json\n",
      " - adapter_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "# Build the model, load checkpoint, and save only the PEFT adapter\n",
    "out_dir = adapter_dir\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model = build_model(BASE_MODEL)\n",
    "_missing, _unexpected = load_checkpoint_into_model(model, ckpt_path)\n",
    "\n",
    "# Save the PEFT adapter (LoRA weights)\n",
    "model.base_model.save_pretrained(out_dir)\n",
    "print(f'\\u2713 Saved PEFT adapter to: {out_dir}')\n",
    "\n",
    "# List saved files\n",
    "print('Saved files:')\n",
    "for p in sorted(out_dir.glob('*')):\n",
    "    print(' -', p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5abfd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected metrics: {}\n",
      "Model Card written to: /Users/edwardhuang/Documents/GitHub/models/peft_adapters/fairness_lora/README.md\n",
      "\n",
      "--- README preview ---\n",
      "---\n",
      "tags:\n",
      "  - lora\n",
      "  - peft\n",
      "  - fairness\n",
      "  - resume-matching\n",
      "  - retrieval\n",
      "  - sentence-similarity\n",
      "library_name: peft\n",
      "base_model: BAAI/bge-large-en-v1.5\n",
      "pipeline_tag: sentence-similarity\n",
      "language:\n",
      "  - en\n",
      "---\n",
      "\n",
      "# renhehuang/fair-resume-job-matcher-lora\n",
      "\n",
      "Fairness-aware LoRA adapter for resumeâ€“job matching built on top of `BAAI/bge-large-en-v1.5`.\n",
      "\n",
      "This adapter was trained with adversarial debiasing and multi-task objectives to reduce group disparities while maintaining utility.\n",
      "\n",
      "## Model Summary\n",
      "- Base model: `BAAI/bge-large-en-v1.5`\n",
      "- Adapter type: LoRA (PEFT)\n",
      "- Task: Resumeâ€“job text similarity (cosine over mean-pooled, L2-normalized embeddings; optional sigmoid for probability)\n",
      "- Intended audience: Researchers and practitioners exploring fairness-aware matching\n",
      "\n",
      "## Quick Start\n",
      "\n",
      "```python\n",
      "from transformers import AutoTokenizer, AutoModel\n",
      "from peft import PeftModel\n",
      "import torch, torch.nn.functional as F\n",
      "\n",
      "BASE = \"BAAI/bge-large-en-v1.5\"\n",
      "ADAPTER = \"renhehuang/fair-resume-job-matcher-lora\"  \n"
     ]
    }
   ],
   "source": [
    "# Generate Model Card (README.md) with metrics if available\n",
    "import importlib, scripts.export_fair_lora_to_hf as exp\n",
    "importlib.reload(exp)\n",
    "from scripts.export_fair_lora_to_hf import write_model_card, maybe_collect_metrics\n",
    "\n",
    "metrics_csv = (_repo_root / 'models' / 'fair_adversarial' / 'epoch_metrics.csv').resolve()\n",
    "metrics = maybe_collect_metrics(metrics_csv)\n",
    "print('Collected metrics:', json.dumps(metrics, indent=2))\n",
    "\n",
    "# Use repo id if provided; else fallback to folder name\n",
    "repo_for_card = REPO_ID or adapter_dir.name\n",
    "write_model_card(adapter_dir, repo_for_card, BASE_MODEL, 'fairness-lora', metrics)\n",
    "\n",
    "readme_path = adapter_dir / 'README.md'\n",
    "print('Model Card written to:', readme_path)\n",
    "print('\\n--- README preview ---')\n",
    "preview = readme_path.read_text(encoding='utf-8')\n",
    "print(preview[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26a941",
   "metadata": {},
   "source": [
    "## Optional: Login and prepare to push\n",
    "- Fill `REPO_ID` in the config cell (e.g., `your-username/fair-resume-matcher-lora`).\n",
    "- Toggle `DO_PUSH = True` if you want to upload.\n",
    "- If you haven't logged in from this environment, run the cell below to login via a widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f6799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='')  # Uncomment to login via a widget in notebook environments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e026a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b3796db4b74036a5cbf5c3cf0bec32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/28.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Uploaded adapter to https://huggingface.co/renhehuang/fair-resume-job-matcher-lora\n"
     ]
    }
   ],
   "source": [
    "# Push to Hugging Face Hub (if enabled)\n",
    "if DO_PUSH:\n",
    "    if not REPO_ID:\n",
    "        raise ValueError('DO_PUSH is True but REPO_ID is empty. Please set REPO_ID first.')\n",
    "    push_to_hub(Path(ADAPTER_DIR), REPO_ID, private=PRIVATE)\n",
    "else:\n",
    "    print('Skipping push (set DO_PUSH=True to upload).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60c6f6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using adapter at: /Users/edwardhuang/Documents/GitHub/bge-lora-fairness-finetuning/models/peft_adapters/fairness_lora\n",
      "{'cosine': 0.8230911493301392, 'prob': 0.6948921084403992}\n",
      "{'cosine': 0.8230911493301392, 'prob': 0.6948921084403992}\n"
     ]
    }
   ],
   "source": [
    "# Smoke test: load adapter locally and compute a cosine score\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "adapter_path = str(adapter_dir)\n",
    "print('Using adapter at:', adapter_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "base = AutoModel.from_pretrained(BASE_MODEL)\n",
    "model = PeftModel.from_pretrained(base, adapter_path)\n",
    "model.eval()\n",
    "\n",
    "def encode(text: str):\n",
    "    enc = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=256)\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "        emb = F.normalize(out.last_hidden_state.mean(dim=1), p=2, dim=1)\n",
    "    return emb\n",
    "\n",
    "a = 'Software engineer with Python experience.'\n",
    "b = 'Hiring backend Python developer with API experience.'\n",
    "cos = (encode(a) * encode(b)).sum(dim=1).item()\n",
    "prob = torch.sigmoid(torch.tensor(cos)).item()\n",
    "print({'cosine': cos, 'prob': prob})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
