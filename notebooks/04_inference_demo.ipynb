{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3bfc11e",
   "metadata": {},
   "source": [
    "# Inference Demo: Fair LoRA Resume-Job Matching\n",
    "\n",
    "本 Notebook 示範如何載入 BGE base + LoRA adapter 或訓練後的本地 checkpoint，計算履歷與職位描述的匹配分數 (match score)。\n",
    "\n",
    "## 內容\n",
    "1. 環境與匯入\n",
    "2. 模型與 tokenizer 載入 (遠端 Adapter 或本地最佳模型)\n",
    "3. 單 pair 推論\n",
    "4. 批次多 pair 推論\n",
    "5. (選用) 敏感學校名稱遮蔽\n",
    "6. 匹配分數解讀\n",
    "匹配分數流程：\n",
    "- 取最後隱層輸出，mean pooling 或 CLS pooling。\n",
    "- L2 normalize 後點積相似度。\n",
    "- sigmoid 將相似度壓縮到 (0,1)。\n",
    "> 如果你已訓練並產生 `best_util_model.pt` 或 `best_fairness_model.pt`，可直接載入該檔案 (包含完整模型 state_dict)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e0496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports & basic setup\n",
    "import os, json, math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Try optional peft import; fall back gracefully if missing\n",
    "try:\n",
    "    from peft import PeftModel  # type: ignore\n",
    "except Exception as e:\n",
    "    PeftModel = None  # allows running without adapter\n",
    "    print(\"peft not installed; remote LoRA adapter loading will be skipped. Install with `pip install peft`. \")\n",
    "\n",
    "DEVICE = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "BASE_MODEL = 'BAAI/bge-large-en-v1.5'\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7b9b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "# 2. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "# Optionally restrict max length for speed/memory:\n",
    "tokenizer.model_max_length = 256\n",
    "print('Tokenizer loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdef575",
   "metadata": {},
   "source": [
    "## 選擇載入方式\n",
    "- Option A: Hugging Face Hub 上的 LoRA adapter (需要 adapter repo)。\n",
    "- Option B: 使用訓練流程產生的本地 checkpoint (`models/fair_adversarial/best_util_model.pt`)。\n",
    "\n",
    "若使用 Option B，因訓練代碼採用自訂 `FairLoRAModel`，這裡示範一個輕量的推論版本，直接使用 base model + pooling。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "123ae997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f1171bfe434baf9540ef4ecd2cdc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61aed500b1a4f4e87457599ae2d3a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/4.74M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded remote LoRA adapter: shashu2325/resume-job-matcher-lora\n",
      "Model ready.\n",
      "Model ready.\n"
     ]
    }
   ],
   "source": [
    "# 3A. Load base + remote LoRA adapter (if available)\n",
    "USE_REMOTE_ADAPTER = True  # 改 True 如果要使用 HuggingFace 上的 LoRA adapter\n",
    "REMOTE_ADAPTER_PATH = 'shashu2325/resume-job-matcher-lora'  # 範例 adapter repo\n",
    "\n",
    "base_model = AutoModel.from_pretrained(BASE_MODEL, torch_dtype=torch.float32)\n",
    "if USE_REMOTE_ADAPTER and PeftModel is not None:\n",
    "    try:\n",
    "        model = PeftModel.from_pretrained(base_model, REMOTE_ADAPTER_PATH)\n",
    "        print(f\"Loaded remote LoRA adapter: {REMOTE_ADAPTER_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load remote adapter: {e}; falling back to base model.\")\n",
    "        model = base_model\n",
    "else:\n",
    "    model = base_model\n",
    "    if USE_REMOTE_ADAPTER and PeftModel is None:\n",
    "        print(\"peft not available; using base model without adapter.\")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print('Model ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd11a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_CKPT_PATH = Path('models/fair_adversarial/best_fairness_model.pt')\n",
    "if LOCAL_CKPT_PATH.exists():\n",
    "    print(f'Loading local checkpoint from {LOCAL_CKPT_PATH}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5f8f781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded local checkpoint: ../models/fair_adversarial/best_fairness_model.pt\n",
      "  Missing keys: 535 | Unexpected keys: 715\n"
     ]
    }
   ],
   "source": [
    "# 3B. (Optional) Load local fairness-aware checkpoint weights if available\n",
    "# This assumes the checkpoint contains a 'model_state_dict' compatible with the underlying base model.\n",
    "LOCAL_CKPT_PATH = Path('../models/fair_adversarial/best_fairness_model.pt')  # or best_fairness_model.pt\n",
    "if LOCAL_CKPT_PATH.exists():\n",
    "    try:\n",
    "        ckpt = torch.load(LOCAL_CKPT_PATH, map_location=DEVICE)\n",
    "        state = ckpt.get('model_state_dict', None)\n",
    "        if state is None and isinstance(ckpt, dict):\n",
    "            # sometimes saved as plain state_dict\n",
    "            state = ckpt\n",
    "        if state is not None:\n",
    "            missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "            print(f'Loaded local checkpoint: {LOCAL_CKPT_PATH}')\n",
    "            print('  Missing keys:', len(missing), '| Unexpected keys:', len(unexpected))\n",
    "        else:\n",
    "            print('Checkpoint found but no model_state_dict; skipped loading.')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to load local checkpoint: {e}')\n",
    "else:\n",
    "    print('Local checkpoint not found; using current model weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029153e",
   "metadata": {},
   "source": [
    "## 4. Helper: Text -> Embedding\n",
    "采用 mean pooling + L2 normalize。可依需求改成 CLS token 或加權 pooling。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9578d8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder helpers ready.\n"
     ]
    }
   ],
   "source": [
    "def encode_text(text: str, max_length: int = 256):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n",
    "    inputs = {k: v.to(DEVICE) for k,v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden = outputs.last_hidden_state  # (batch, seq, dim)\n",
    "        emb = hidden.mean(dim=1)  # mean pooling\n",
    "        emb = F.normalize(emb, p=2, dim=1)\n",
    "    return emb.cpu()  # return on CPU for similarity ops\n",
    "\n",
    "def match_score(resume_text: str, job_text: str) -> float:\n",
    "    r_emb = encode_text(resume_text)\n",
    "    j_emb = encode_text(job_text)\n",
    "    # cosine similarity (since normalized) == dot product\n",
    "    sim = torch.sum(r_emb * j_emb, dim=1)\n",
    "    # Optionally transform to (0,1) via sigmoid; or just use raw cosine similarity\n",
    "    return torch.sigmoid(sim).item(), sim.item()\n",
    "\n",
    "print('Encoder helpers ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37df3ef",
   "metadata": {},
   "source": [
    "## 5. Single Pair Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c83dfd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match probability (sigmoid of cosine): 0.6984\n",
      "Raw cosine similarity: 0.8399\n"
     ]
    }
   ],
   "source": [
    "resume_text = 'Software engineer with Python experience building backend services and APIs.'\n",
    "job_text = 'Seeking a backend Python developer to design scalable microservices.'\n",
    "prob_score, raw_cosine = match_score(resume_text, job_text)\n",
    "print(f'Match probability (sigmoid of cosine): {prob_score:.4f}')\n",
    "print(f'Raw cosine similarity: {raw_cosine:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e00ae",
   "metadata": {},
   "source": [
    "## 6. Batch Multiple Pairs\n",
    "可一次比較多組 pair，方便做快速排名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd6fe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume: Data scientist experienced in NLP and transformers...\n",
      "Job: Looking for ML engineer with NLP background...\n",
      "  prob=0.6916 | cosine=0.8077\n",
      "\n",
      "Resume: Frontend developer skilled in React and TypeScript...\n",
      "Job: Need React engineer for UI component development...\n",
      "  prob=0.6887 | cosine=0.7941\n",
      "\n",
      "Resume: Project manager with agile certification....\n",
      "Job: Hiring Scrum master for cross-team coordination...\n",
      "  prob=0.6643 | cosine=0.6824\n",
      "\n",
      "Resume: Graphic designer using Figma and Adobe suite....\n",
      "Job: Seeking UX/UI designer to craft product interfaces...\n",
      "  prob=0.6543 | cosine=0.6378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('Data scientist experienced in NLP and transformers.', 'Looking for ML engineer with NLP background'),\n",
    "    ('Frontend developer skilled in React and TypeScript.', 'Need React engineer for UI component development'),\n",
    "    ('Project manager with agile certification.', 'Hiring Scrum master for cross-team coordination'),\n",
    "    ('Graphic designer using Figma and Adobe suite.', 'Seeking UX/UI designer to craft product interfaces')\n",
    "]\n",
    "results = []\n",
    "for r, j in pairs:\n",
    "    prob, cosine = match_score(r, j)\n",
    "    results.append({'resume': r[:50]+'...', 'job': j[:50]+'...', 'prob_score': prob, 'cosine': cosine})\n",
    "\n",
    "for row in results:\n",
    "    print(f\"Resume: {row['resume']}\\nJob: {row['job']}\\n  prob={row['prob_score']:.4f} | cosine={row['cosine']:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
