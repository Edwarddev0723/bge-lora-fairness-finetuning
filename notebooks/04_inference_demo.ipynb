{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3bfc11e",
   "metadata": {},
   "source": [
    "# Inference Demo: Fair LoRA Resume-Job Matching\n",
    "\n",
    "本 Notebook 示範如何載入 BGE base + LoRA adapter 或訓練後的本地 checkpoint，計算履歷與職位描述的匹配分數 (match score)。\n",
    "\n",
    "## 內容\n",
    "1. 環境與匯入\n",
    "2. 模型與 tokenizer 載入 (遠端 Adapter 或本地最佳模型)\n",
    "3. 單 pair 推論\n",
    "4. 批次多 pair 推論\n",
    "5. (選用) 敏感學校名稱遮蔽\n",
    "6. 匹配分數解讀\n",
    "匹配分數流程：\n",
    "- 取最後隱層輸出，mean pooling 或 CLS pooling。\n",
    "- L2 normalize 後點積相似度。\n",
    "- sigmoid 將相似度壓縮到 (0,1)。\n",
    "> 如果你已訓練並產生 `best_util_model.pt` 或 `best_fairness_model.pt`，可直接載入該檔案 (包含完整模型 state_dict)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports & basic setup\n",
    "import os, json, math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Try optional peft import; fall back gracefully if missing\n",
    "try:\n",
    "    from peft import PeftModel  # type: ignore\n",
    "except Exception as e:\n",
    "    PeftModel = None  # allows running without adapter\n",
    "    print(\"peft not installed; remote LoRA adapter loading will be skipped. Install with `pip install peft`. \")\n",
    "\n",
    "DEVICE = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "BASE_MODEL = 'BAAI/bge-large-en-v1.5'\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b9b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "# Optionally restrict max length for speed/memory:\n",
    "tokenizer.model_max_length = 256\n",
    "print('Tokenizer loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdef575",
   "metadata": {},
   "source": [
    "## 選擇載入方式\n",
    "- Option A: Hugging Face Hub 上的 LoRA adapter (需要 adapter repo)。\n",
    "- Option B: 使用訓練流程產生的本地 checkpoint (`models/fair_adversarial/best_util_model.pt`)。\n",
    "\n",
    "若使用 Option B，因訓練代碼採用自訂 `FairLoRAModel`，這裡示範一個輕量的推論版本，直接使用 base model + pooling。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ae997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3A. Load base + remote LoRA adapter (if available)\n",
    "USE_REMOTE_ADAPTER = False  # 改 True 如果要使用 HuggingFace 上的 LoRA adapter\n",
    "REMOTE_ADAPTER_PATH = 'shashu2325/resume-job-matcher-lora'  # 範例 adapter repo\n",
    "\n",
    "base_model = AutoModel.from_pretrained(BASE_MODEL, torch_dtype=torch.float32)\n",
    "if USE_REMOTE_ADAPTER and PeftModel is not None:\n",
    "    try:\n",
    "        model = PeftModel.from_pretrained(base_model, REMOTE_ADAPTER_PATH)\n",
    "        print(f\"Loaded remote LoRA adapter: {REMOTE_ADAPTER_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load remote adapter: {e}; falling back to base model.\")\n",
    "        model = base_model\n",
    "else:\n",
    "    model = base_model\n",
    "    if USE_REMOTE_ADAPTER and PeftModel is None:\n",
    "        print(\"peft not available; using base model without adapter.\")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print('Model ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3B. (Optional) Load local fairness-aware checkpoint weights if available\n",
    "# This assumes the checkpoint contains a 'model_state_dict' compatible with the underlying base model.\n",
    "LOCAL_CKPT_PATH = Path('models/fair_adversarial/best_util_model.pt')  # or best_fairness_model.pt\n",
    "if LOCAL_CKPT_PATH.exists():\n",
    "    try:\n",
    "        ckpt = torch.load(LOCAL_CKPT_PATH, map_location=DEVICE)\n",
    "        state = ckpt.get('model_state_dict', None)\n",
    "        if state is None and isinstance(ckpt, dict):\n",
    "            # sometimes saved as plain state_dict\n",
    "            state = ckpt\n",
    "        if state is not None:\n",
    "            missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "            print(f'Loaded local checkpoint: {LOCAL_CKPT_PATH}')\n",
    "            print('  Missing keys:', len(missing), '| Unexpected keys:', len(unexpected))\n",
    "        else:\n",
    "            print('Checkpoint found but no model_state_dict; skipped loading.')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to load local checkpoint: {e}')\n",
    "else:\n",
    "    print('Local checkpoint not found; using current model weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029153e",
   "metadata": {},
   "source": [
    "## 4. Helper: Text -> Embedding\n",
    "采用 mean pooling + L2 normalize。可依需求改成 CLS token 或加權 pooling。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text: str, max_length: int = 256):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n",
    "    inputs = {k: v.to(DEVICE) for k,v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden = outputs.last_hidden_state  # (batch, seq, dim)\n",
    "        emb = hidden.mean(dim=1)  # mean pooling\n",
    "        emb = F.normalize(emb, p=2, dim=1)\n",
    "    return emb.cpu()  # return on CPU for similarity ops\n",
    "\n",
    "def match_score(resume_text: str, job_text: str) -> float:\n",
    "    r_emb = encode_text(resume_text)\n",
    "    j_emb = encode_text(job_text)\n",
    "    # cosine similarity (since normalized) == dot product\n",
    "    sim = torch.sum(r_emb * j_emb, dim=1)\n",
    "    # Optionally transform to (0,1) via sigmoid; or just use raw cosine similarity\n",
    "    return torch.sigmoid(sim).item(), sim.item()\n",
    "\n",
    "print('Encoder helpers ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37df3ef",
   "metadata": {},
   "source": [
    "## 5. Single Pair Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83dfd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = 'Software engineer with Python experience building backend services and APIs.'\n",
    "job_text = 'Seeking a backend Python developer to design scalable microservices.'\n",
    "prob_score, raw_cosine = match_score(resume_text, job_text)\n",
    "print(f'Match probability (sigmoid of cosine): {prob_score:.4f}')\n",
    "print(f'Raw cosine similarity: {raw_cosine:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e00ae",
   "metadata": {},
   "source": [
    "## 6. Batch Multiple Pairs\n",
    "可一次比較多組 pair，方便做快速排名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('Data scientist experienced in NLP and transformers.', 'Looking for ML engineer with NLP background'),\n",
    "    ('Frontend developer skilled in React and TypeScript.', 'Need React engineer for UI component development'),\n",
    "    ('Project manager with agile certification.', 'Hiring Scrum master for cross-team coordination'),\n",
    "    ('Graphic designer using Figma and Adobe suite.', 'Seeking UX/UI designer to craft product interfaces')\n",
    "]\n",
    "results = []\n",
    "for r, j in pairs:\n",
    "    prob, cosine = match_score(r, j)\n",
    "    results.append({'resume': r[:50]+'...', 'job': j[:50]+'...', 'prob_score': prob, 'cosine': cosine})\n",
    "\n",
    "for row in results:\n",
    "    print(f\"Resume: {row['resume']}\\nJob: {row['job']}\\n  prob={row['prob_score']:.4f} | cosine={row['cosine']:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
